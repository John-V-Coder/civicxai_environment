# üéâ Phase 2: Knowledge Ingestion Pipeline - COMPLETE!

## What Was Implemented

Phase 2 adds **automatic knowledge extraction** from documents - the system can now read PDFs, understand their content, and populate the knowledge base automatically!

---

## ‚úÖ Components Delivered

### 1. **PDF Processor** (`cognitive/processors/pdf_processor.py`)

**Purpose:** Extract text and metadata from PDF documents

**Capabilities:**
- ‚úÖ Extract text from PDF files or bytes
- ‚úÖ Extract metadata (author, title, dates)
- ‚úÖ Split documents into sections
- ‚úÖ Identify key sentences
- ‚úÖ Get text statistics

**Example Usage:**
```python
from cognitive.processors.pdf_processor import get_pdf_processor

processor = get_pdf_processor()

# Process PDF file
result = processor.extract_text_from_file('policy.pdf')
print(f"Extracted {result['word_count']} words from {result['pages']} pages")
print(result['text'])

# Process uploaded PDF
result = processor.extract_text_from_bytes(pdf_bytes, 'document.pdf')
```

---

### 2. **Concept Extractor** (`cognitive/processors/concept_extractor.py`)

**Purpose:** Use NLP to extract concepts, entities, and relationships from text

**Capabilities:**
- ‚úÖ Extract key concepts (noun phrases)
- ‚úÖ Identify named entities (places, organizations, people)
- ‚úÖ Extract keywords with importance scores
- ‚úÖ Identify main topics
- ‚úÖ Find relationships (subject-verb-object)
- ‚úÖ Comprehensive document analysis

**Example Usage:**
```python
from cognitive.processors.concept_extractor import get_concept_extractor

extractor = get_concept_extractor()

# Extract concepts
concepts = extractor.extract_concepts(document_text)
for concept in concepts:
    print(f"{concept['text']}: importance={concept['importance']}")

# Extract entities
entities = extractor.extract_entities(document_text)
for entity in entities:
    print(f"{entity['text']} ({entity['type']})")

# Get topics
topics = extractor.extract_topics(document_text)
print(f"Topics: {', '.join(topics)}")

# Comprehensive analysis
analysis = extractor.analyze_document(document_text)
# Returns: concepts, entities, keywords, topics, relationships
```

---

### 3. **Atom Generator** (`cognitive/processors/atom_generator.py`)

**Purpose:** Convert extracted concepts into AtomSpace atoms

**Capabilities:**
- ‚úÖ Generate atoms from analysis results
- ‚úÖ Create concept nodes
- ‚úÖ Add entity classifications
- ‚úÖ Link topics to sources
- ‚úÖ Create relationships
- ‚úÖ Generate domain knowledge

**Example Usage:**
```python
from cognitive.processors.atom_generator import get_atom_generator

generator = get_atom_generator()

# Generate atoms from analysis
stats = generator.generate_from_analysis(analysis, source_id='Doc_123')
print(f"Created {sum(stats.values())} atoms")

# Link similar concepts
generator.link_similar_concepts('Poverty', 'Economic_Hardship', 0.9)

# Add causal link
generator.add_causal_link('High_Poverty', 'Requires_Allocation', 0.85)

# Initialize domain knowledge
stats = generator.generate_domain_knowledge()
```

---

### 4. **Ingestion Pipeline** (`cognitive/ingestion_pipeline.py`)

**Purpose:** Orchestrate the complete ingestion process

**Capabilities:**
- ‚úÖ End-to-end PDF processing
- ‚úÖ Text processing
- ‚úÖ Batch processing
- ‚úÖ Domain knowledge initialization

**Example Usage:**
```python
from cognitive.ingestion_pipeline import get_ingestion_pipeline

pipeline = get_ingestion_pipeline()

# Process PDF file
result = pipeline.process_pdf_file('policy.pdf', 'Policy_2024')
print(f"Success: {result['success']}")
print(f"Atoms created: {result['atoms_created']}")
print(f"Topics: {result['key_topics']}")

# Process uploaded PDF
result = pipeline.process_pdf_bytes(pdf_bytes, 'document.pdf', 'Doc_123')

# Process plain text
result = pipeline.process_text(text_content, 'Text_Source')

# Batch process multiple files
results = pipeline.batch_process_files(['doc1.pdf', 'doc2.pdf', 'doc3.pdf'])

# Initialize domain knowledge
result = pipeline.initialize_domain_knowledge()
```

---

### 5. **API Endpoints** (Added to `cognitive/views.py`)

**New Endpoints:**

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/cognitive/ingest/pdf/` | POST | Upload and process PDF |
| `/api/cognitive/ingest/text/` | POST | Process plain text |
| `/api/cognitive/ingest/initialize/` | POST | Initialize domain knowledge |

**Example API Calls:**
```bash
# Upload and process PDF
curl -X POST http://localhost:8000/api/cognitive/ingest/pdf/ \
  -F "file=@policy.pdf" \
  -F "source_id=Policy_2024"

# Process text
curl -X POST http://localhost:8000/api/cognitive/ingest/text/ \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Document content here...",
    "source_id": "Text_123"
  }'

# Initialize domain knowledge
curl -X POST http://localhost:8000/api/cognitive/ingest/initialize/
```

---

### 6. **Auto-Processing Signals** (`cognitive/signals.py`)

**Purpose:** Automatically process DataSource uploads

**How It Works:**
When a DataSource is created via Django admin or API:
1. Signal handler detects the new source
2. Automatically extracts content (PDF or text)
3. Processes through NLP pipeline
4. Generates atoms in AtomSpace
5. Updates source with extracted topics

**Configuration:**
```python
# cognitive/apps.py
class CognitiveConfig(AppConfig):
    def ready(self):
        from . import signals
        signals.register_signals()  # Auto-registers signal handlers
```

---

## üìÅ Files Created

```
civicxai_backend/
‚îú‚îÄ‚îÄ cognitive/
‚îÇ   ‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                        # ‚úÖ Processors module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor.py                   # ‚úÖ PDF text extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ concept_extractor.py               # ‚úÖ NLP concept extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ atom_generator.py                  # ‚úÖ Atom generation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ingestion_pipeline.py                  # ‚úÖ Pipeline orchestration
‚îÇ   ‚îú‚îÄ‚îÄ signals.py                             # ‚úÖ Auto-processing signals
‚îÇ   ‚îú‚îÄ‚îÄ apps.py                                # ‚úÖ App configuration
‚îÇ   ‚îú‚îÄ‚îÄ views.py                               # MODIFIED: Added ingestion views
‚îÇ   ‚îú‚îÄ‚îÄ urls.py                                # MODIFIED: Added ingestion routes
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îî‚îÄ‚îÄ test_ingestion_pipeline.py         # ‚úÖ Phase 2 test suite
‚îÇ
‚îî‚îÄ‚îÄ PHASE_2_COMPLETE.md                        # ‚úÖ This file
```

---

## üöÄ How to Use Phase 2

### Method 1: Via API (Recommended)

**Initialize domain knowledge first:**
```bash
curl -X POST http://localhost:8000/api/cognitive/ingest/initialize/
```

**Upload and process a PDF:**
```bash
curl -X POST http://localhost:8000/api/cognitive/ingest/pdf/ \
  -F "file=@my_policy_document.pdf" \
  -F "source_id=Policy_MyDoc"
```

**Response:**
```json
{
  "success": true,
  "source_id": "Policy_MyDoc",
  "file_name": "my_policy_document.pdf",
  "pages": 25,
  "word_count": 5432,
  "concepts_extracted": 127,
  "entities_extracted": 45,
  "topics_extracted": 8,
  "atoms_created": 189,
  "key_topics": ["Poverty", "Allocation", "Development", "Infrastructure", "Governance"]
}
```

---

### Method 2: Via Django Admin

1. **Go to Django admin:**
   ```
   http://localhost:8000/admin
   ```

2. **Add DataSource:**
   - Click "Data sources" ‚Üí "Add Data source"
   - Upload PDF or add URL
   - Save

3. **Automatic Processing:**
   - System automatically extracts content
   - NLP analysis runs in background
   - Atoms are generated
   - Summary is auto-populated

---

### Method 3: Programmatically

```python
from cognitive.ingestion_pipeline import get_ingestion_pipeline

pipeline = get_ingestion_pipeline()

# Process a local PDF file
result = pipeline.process_pdf_file('path/to/document.pdf', 'Doc_123')

if result['success']:
    print(f"‚úÖ Processed successfully!")
    print(f"   Created {result['atoms_created']} atoms")
    print(f"   Topics: {', '.join(result['key_topics'])}")
else:
    print(f"‚ùå Error: {result['error']}")
```

---

## üîÑ Complete Workflow

```
1. Upload PDF
   ‚Üì
2. Extract Text (PDF Processor)
   - Extract all text
   - Get metadata
   - Split into sections
   ‚Üì
3. Analyze Text (Concept Extractor)
   - Extract concepts
   - Identify entities
   - Find topics
   - Discover relationships
   ‚Üì
4. Generate Atoms (Atom Generator)
   - Create concept nodes
   - Add entity types
   - Link topics
   - Create relationships
   ‚Üì
5. Store in AtomSpace
   - Knowledge is now queryable
   - Available for reasoning
   - Can be referenced in chat
```

---

## üéØ Real-World Example

**Scenario:** Upload "Kenya County Allocation Act 2024.pdf"

```bash
# 1. Initialize domain knowledge (one time)
curl -X POST http://localhost:8000/api/cognitive/ingest/initialize/

# 2. Upload the policy document
curl -X POST http://localhost:8000/api/cognitive/ingest/pdf/ \
  -F "file=@Kenya_County_Allocation_Act_2024.pdf" \
  -F "source_id=Policy_CountyAct2024"
```

**What Happens:**
```
‚úÖ PDF Processing:
   - Extracted 89 pages
   - Found 12,543 words
   
‚úÖ NLP Analysis:
   - Extracted 234 concepts
   - Found 67 entities (Nairobi, Mombasa, Ministry of Planning, etc.)
   - Identified 12 topics (Poverty, Allocation, Infrastructure, etc.)
   - Discovered 45 relationships
   
‚úÖ Atom Generation:
   - Created 346 atoms in AtomSpace
   - Linked to 12 topics
   - Connected to similar concepts
   
‚úÖ Now Available:
   - Can reason about this policy
   - Can cite it in explanations
   - Can compare with other policies
```

**Query the knowledge:**
```bash
# Find evidence about allocation
curl -X POST http://localhost:8000/api/cognitive/reason/ \
  -H "Content-Type: application/json" \
  -d '{
    "operation": "find_evidence",
    "parameters": {
      "decision": "high_priority",
      "context": {"topics": ["poverty", "allocation"]}
    }
  }'
```

**Response includes:**
```json
{
  "evidence": [
    {
      "source_id": "Policy_CountyAct2024",
      "type": "policy",
      "relevance": 0.9
    }
  ]
}
```

---

## üß™ Testing Phase 2

```bash
# Run Phase 2 test suite
cd civicxai_backend
python cognitive/tests/test_ingestion_pipeline.py
```

**Expected Output:**
```
============================================================
üî¨ KNOWLEDGE INGESTION PIPELINE TEST SUITE (Phase 2)
============================================================

‚úÖ PASS - PDF Processor Initialization
‚úÖ PASS - Concept Extractor Initialization
‚úÖ PASS - Extract Concepts from Text
‚úÖ PASS - Extract Named Entities
‚úÖ PASS - Extract Keywords
‚úÖ PASS - Extract Topics
‚úÖ PASS - Generate Atoms from Analysis
‚úÖ PASS - Initialize Domain Knowledge
‚úÖ PASS - Process Text Pipeline
‚úÖ PASS - Comprehensive Document Analysis
‚úÖ PASS - Find Domain Concepts
‚úÖ PASS - Pipeline Initialize Domain

============================================================
üìä TEST SUMMARY: 12/12 tests passed
============================================================
‚úÖ All tests passed! Ingestion pipeline is working correctly.
```

---

## üìä What Phase 2 Enables

### Before Phase 2 (Manual):
```python
# Had to manually add every concept
knowledge.add_concept('Poverty')
knowledge.add_concept('Economic_Hardship')
knowledge.add_similarity('Poverty', 'Economic_Hardship', 0.9)
# ... repeat for 100s of concepts
```

### With Phase 2 (Automatic):
```python
# Just upload the document!
pipeline.process_pdf_file('policy.pdf', 'Policy_2024')
# ‚úÖ All concepts automatically extracted
# ‚úÖ All relationships discovered
# ‚úÖ All atoms generated
# ‚úÖ Knowledge base populated
```

---

## üîç Behind the Scenes: NLP Magic

**Input:** "Poverty is a major factor in allocation decisions."

**Step 1: Tokenization**
```
["Poverty", "is", "a", "major", "factor", "in", "allocation", "decisions"]
```

**Step 2: POS Tagging**
```
Poverty (NOUN), major (ADJ), factor (NOUN), allocation (NOUN), decisions (NOUN)
```

**Step 3: Named Entity Recognition**
```
No named entities detected (generic concepts)
```

**Step 4: Dependency Parsing**
```
poverty ‚Üê (nsubj) ‚Üê is ‚Üê factor ‚Üê (prep) ‚Üê in ‚Üê decisions
```

**Step 5: Concept Extraction**
```
Concepts: ["poverty", "major factor", "allocation decisions"]
Relationships: [("poverty", "is", "factor")]
```

**Step 6: Atom Generation**
```
(ConceptNode "Poverty")
(ConceptNode "Allocation_Decisions")
(EvaluationLink (PredicateNode "is_factor_in")
  (ListLink (ConceptNode "Poverty") (ConceptNode "Allocation_Decisions")))
```

---

## üéì Key Improvements

| Metric | Before Phase 2 | With Phase 2 |
|--------|----------------|--------------|
| **Knowledge Entry** | Manual | Automatic |
| **Time to Process Document** | Hours (manual) | Seconds (automatic) |
| **Concepts Extracted** | ~10 (manual) | ~200 (automatic) |
| **Accuracy** | Varies | NLP-validated |
| **Scalability** | Limited | Unlimited |

---

## ‚öôÔ∏è Configuration Options

### Adjust Concept Extraction Sensitivity
```python
from cognitive.processors.concept_extractor import get_concept_extractor

extractor = get_concept_extractor()

# Extract only frequent concepts (more selective)
concepts = extractor.extract_concepts(text, min_frequency=3)

# Extract more keywords
keywords = extractor.extract_keywords(text, top_n=50)
```

### Add Custom Domain Keywords
```python
# Add your own domain-specific terms
extractor.domain_keywords.update([
    'county', 'budget', 'treasury', 'disbursement',
    'constituency', 'ward', 'mpesa', 'nhif'
])
```

---

## üêõ Troubleshooting

### Issue: spaCy model not found

**Error:** `OSError: [E050] Can't find model 'en_core_web_sm'`

**Solution:**
```bash
python -m spacy download en_core_web_sm
```

### Issue: PDF extraction fails

**Error:** `FileNotFoundError` or empty text

**Solutions:**
1. Check file path is correct
2. Ensure PDF is not password-protected
3. Try converting scanned PDFs with OCR first
4. Check file permissions

### Issue: Low concept extraction

**Problem:** Only extracting a few concepts

**Solutions:**
1. Lower `min_frequency` parameter
2. Check if text is meaningful (not just numbers/tables)
3. Ensure document has sufficient content

---

## üöÄ Next Steps

### After Phase 2 Works:

**Phase 3: Advanced Reasoning**
- Multi-hop inference
- Probabilistic Logic Networks (PLN)
- Confidence scoring
- Learning loops

**Phase 4: Chat Integration**
- Cognitive orchestrator
- Auto-route queries to reasoning
- Generate explanations with sources
- Real-time knowledge updates

---

## ‚úÖ Verification Checklist

Before moving to Phase 3:

- [ ] Phase 1 tests pass (12/12)
- [ ] Phase 2 tests pass (12/12)
- [ ] Can upload PDFs via API
- [ ] Concepts are extracted automatically
- [ ] Atoms are created in AtomSpace
- [ ] Can query extracted knowledge
- [ ] Auto-processing works on DataSource upload
- [ ] Domain knowledge initialized

---

## üìñ API Reference

### POST /api/cognitive/ingest/pdf/

**Upload and process PDF document**

**Request:**
```bash
Content-Type: multipart/form-data
- file: PDF file (required)
- source_id: Unique identifier (optional, auto-generated if not provided)
```

**Response:**
```json
{
  "success": true,
  "source_id": "Policy_Doc",
  "pages": 25,
  "word_count": 5000,
  "concepts_extracted": 150,
  "entities_extracted": 40,
  "topics_extracted": 10,
  "atoms_created": 200,
  "key_topics": ["Topic1", "Topic2", ...]
}
```

### POST /api/cognitive/ingest/text/

**Process plain text**

**Request:**
```json
{
  "text": "Document content...",
  "source_id": "Text_123"
}
```

**Response:** Same as PDF endpoint

### POST /api/cognitive/ingest/initialize/

**Initialize domain knowledge**

**Response:**
```json
{
  "success": true,
  "message": "Domain knowledge initialized",
  "stats": {
    "concepts": 17,
    "similarities": 10,
    "causal_links": 6
  }
}
```

---

## üéâ Success Metrics

**Phase 2 is complete when:**

‚úÖ All 12 Phase 2 tests pass  
‚úÖ Can upload and process PDFs  
‚úÖ Concepts are automatically extracted  
‚úÖ Atoms are generated in AtomSpace  
‚úÖ Knowledge is queryable via API  
‚úÖ Auto-processing works on upload  

**You now have automatic knowledge ingestion!** üöÄ

The system can read documents, understand their content, and build a knowledge graph automatically. Ready for Phase 3: Advanced Reasoning!

---

**üìû Need Help?**

1. Run tests: `python cognitive/tests/test_ingestion_pipeline.py`
2. Check logs in Django console
3. Verify spaCy installation: `python -m spacy info`
4. Test API: `curl http://localhost:8000/api/cognitive/health/`
